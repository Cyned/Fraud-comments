{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_profiling as pp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from os.path import join as path_join\n",
    "\n",
    "from user_agents import parse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/'\n",
    "\n",
    "train = pd.read_csv(path_join(DATA_DIR, 'train.csv'), parse_dates=['date_created', 'user_date_created'])\n",
    "test = pd.read_csv(path_join(DATA_DIR,'test.csv'), parse_dates=['date_created', 'user_date_created'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values(by='date_created', ascending=True)\n",
    "\n",
    "train = train.loc[train['date_created'] >= pd.to_datetime('2016-01-01')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_difference_feature(df):\n",
    "    diff = pd.to_datetime(df['date_created']) - pd.to_datetime(df['user_date_created'])\n",
    "    result = diff.apply(lambda x: x.seconds // 60)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_comments(df):\n",
    "    '''merge return dataframe with train/test'''\n",
    "    result = pd.DataFrame(df.groupby('user_id').size().reset_index(name='counts'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dt_next = lambda ts: ts.diff().shift(1)\n",
    "train['from_prev_comment'] = train.groupby('user_id').date_created.apply(get_dt_next).dt.total_seconds() // 60\n",
    "\n",
    "test['from_prev_comment'] = test.groupby('user_id').date_created.apply(get_dt_next).dt.total_seconds() // 60\n",
    "\n",
    "train['from_prev_comment'].fillna(2500000, inplace=True)\n",
    "test['from_prev_comment'].fillna(2500000, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['diff_time_comment_registration'] = time_difference_feature(train)\n",
    "test['diff_time_comment_registration'] = time_difference_feature(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, count_comments(train), how='left', on='user_id')\n",
    "test = pd.merge(test, count_comments(test), how='left', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['count_sent'] = train[\"comment\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "train['count_words'] = train[\"comment\"].apply(lambda x: len(str(x).split()))\n",
    "train['count_unique_words'] = train[\"comment\"].apply(lambda x: len(set(str(x).split())))\n",
    "train['count_letters'] = train[\"comment\"].apply(lambda x: len(str(x)))\n",
    "train[\"count_punctuations\"] = train[\"comment\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "train[\"count_words_upper\"] = train[\"comment\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "train[\"count_words_title\"] = train[\"comment\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "train['word_unique_percent'] = train['count_unique_words'] * 100 / train['count_words']\n",
    "train[\"weekend\"] = ((train[\"date_created\"].dt.dayofweek) // 5 == 1).astype(int)\n",
    "train['month'] = train['date_created'].apply(lambda x: x.month)\n",
    "\n",
    "test['count_sent'] = test[\"comment\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "test['count_words'] = test[\"comment\"].apply(lambda x: len(str(x).split()))\n",
    "test['count_unique_words'] = test[\"comment\"].apply(lambda x: len(set(str(x).split())))\n",
    "test['count_letters'] = test[\"comment\"].apply(lambda x: len(str(x)))\n",
    "test[\"count_punctuations\"] = test[\"comment\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "test[\"count_words_upper\"] = test[\"comment\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test[\"count_words_title\"] = test[\"comment\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test['word_unique_percent'] = test['count_unique_words'] * 100 / test['count_words']\n",
    "test[\"weekend\"] = ((test[\"date_created\"].dt.dayofweek) // 5 == 1).astype(int)\n",
    "test['month'] = test['date_created'].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count amount of every ip for company\n",
    "ip_amount_for_company_train = train.groupby(['company_id', 'user_ip']).user_ip.count()\n",
    "ip_company_train = pd.DataFrame(ip_amount_for_company_train) \n",
    "ip_company_train.columns = ['ip_count_for_company']\n",
    "ip_company_train = ip_company_train.reset_index()\n",
    "train = pd.merge(train, ip_company_train, how='left', on=['company_id', 'user_ip'])\n",
    "train.ip_count_for_company.fillna(1, inplace = True)\n",
    "\n",
    "ip_amount_for_company_test = test.groupby(['company_id', 'user_ip']).user_ip.count()\n",
    "ip_company_test = pd.DataFrame(ip_amount_for_company_test) \n",
    "ip_company_test.columns = ['ip_count_for_company']\n",
    "ip_company_test = ip_company_test.reset_index()\n",
    "test = pd.merge(test, ip_company_test, how='left', on=['company_id', 'user_ip'])\n",
    "test.ip_count_for_company.fillna(1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count amount of every ip for product\n",
    "ip_amount_for_product_train = train.groupby(['product_id', 'user_ip']).user_ip.count()\n",
    "ip_product_train = pd.DataFrame(ip_amount_for_product_train) \n",
    "ip_product_train.columns = ['ip_count_for_product']\n",
    "ip_product_train = ip_product_train.reset_index()\n",
    "train = pd.merge(train, ip_product_train, how='left', on=['product_id', 'user_ip'])\n",
    "train.ip_count_for_product.fillna(1, inplace = True)\n",
    "\n",
    "ip_amount_for_product_test = test.groupby(['product_id', 'user_ip']).user_ip.count()\n",
    "ip_product_test = pd.DataFrame(ip_amount_for_product_test) \n",
    "ip_product_test.columns = ['ip_count_for_product']\n",
    "ip_product_test = ip_product_test.reset_index()\n",
    "test = pd.merge(test, ip_product_test, how='left', on=['product_id', 'user_ip'])\n",
    "test.ip_count_for_product.fillna(1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count amount of every user id for company\n",
    "id_amount_for_company_train = train.groupby(['company_id', 'user_id']).user_id.count()\n",
    "id_company_train = pd.DataFrame(id_amount_for_company_train) \n",
    "id_company_train.columns = ['id_count_for_company']\n",
    "id_company_train = id_company_train.reset_index()\n",
    "train = pd.merge(train, id_company_train, how='left', on=['company_id', 'user_id'])\n",
    "train.id_count_for_company.fillna(1, inplace = True)\n",
    "\n",
    "id_amount_for_company_test = test.groupby(['company_id', 'user_id']).user_id.count()\n",
    "id_company_test = pd.DataFrame(id_amount_for_company_test) \n",
    "id_company_test.columns = ['id_count_for_company']\n",
    "id_company_test = id_company_test.reset_index()\n",
    "test = pd.merge(test, id_company_test, how='left', on=['company_id', 'user_id'])\n",
    "test.id_count_for_company.fillna(1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean comments ammount within max - min date_created\n",
    "\n",
    "train_user_min = pd.DataFrame(train.groupby(['user_id']).date_created.min())\n",
    "train_user_min.columns = ['min_date']\n",
    "train_user_min = train_user_min.reset_index()\n",
    "\n",
    "train_user_max = pd.DataFrame(train.groupby(['user_id']).date_created.max())\n",
    "train_user_max.columns = ['max_date']\n",
    "train_user_max = train_user_max.reset_index()\n",
    "\n",
    "train_user_diff_time = pd.merge(train_user_max, train_user_min, how='left', on='user_id')\n",
    "train_user_diff_time['diff_time'] = (train_user_diff_time.max_date - train_user_diff_time.min_date).dt.total_seconds()//(60*24)\n",
    "train_user_diff_time = pd.merge(train_user_diff_time, count_comments(train), how='left', on='user_id')\n",
    "train_user_diff_time['mean_in_time'] = train_user_diff_time.counts/(train_user_diff_time.diff_time+100)\n",
    "train_user_diff_time.drop(['max_date', 'min_date', 'diff_time', 'counts'], axis = 1, inplace = True)\n",
    "\n",
    "train = pd.merge(train, train_user_diff_time, how='left', on='user_id')\n",
    "\n",
    "\n",
    "test_user_min = pd.DataFrame(test.groupby(['user_id']).date_created.min())\n",
    "test_user_min.columns = ['min_date']\n",
    "test_user_min = test_user_min.reset_index()\n",
    "\n",
    "test_user_max = pd.DataFrame(test.groupby(['user_id']).date_created.max())\n",
    "test_user_max.columns = ['max_date']\n",
    "test_user_max = test_user_max.reset_index()\n",
    "\n",
    "test_user_diff_time = pd.merge(test_user_max, test_user_min, how='left', on='user_id')\n",
    "test_user_diff_time['diff_time'] = (test_user_diff_time.max_date - test_user_diff_time.min_date).dt.total_seconds()//(60*24)\n",
    "test_user_diff_time = pd.merge(test_user_diff_time, count_comments(test), how='left', on='user_id')\n",
    "test_user_diff_time['mean_in_time'] = test_user_diff_time.counts/(test_user_diff_time.diff_time+100)\n",
    "test_user_diff_time.drop(['max_date', 'min_date', 'diff_time', 'counts'], axis = 1, inplace = True)\n",
    "\n",
    "test = pd.merge(test, test_user_diff_time, how='left', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse user-agent feature\n",
    "train['user_agent'].fillna('None', inplace = True)\n",
    "test['user_agent'].fillna('None', inplace = True)\n",
    "\n",
    "ldevice = lambda x: x.device.brand\n",
    "los = lambda x: x.os.family\n",
    "lbrowser = lambda x: x.browser.family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ua = train['user_agent'].apply(parse)\n",
    "train['device'] = train_ua.apply(ldevice)\n",
    "train['os'] = train_ua.apply(los)\n",
    "train['browser'] = train_ua.apply(lbrowser)\n",
    "\n",
    "test_ua = train['user_agent'].apply(parse)\n",
    "test['device'] = test_ua.apply(ldevice)\n",
    "test['os'] = test_ua.apply(los)\n",
    "test['browser'] = test_ua.apply(lbrowser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "columns = ['os', 'device', 'browser']\n",
    "for column in columns:\n",
    "    train_ = train[column].astype(str).str.lower()\n",
    "    test_ = test[column].astype(str).str.lower()\n",
    "    train_test_union = set(train_).union(set(test_))\n",
    "    le.fit(list(train_test_union))\n",
    "    \n",
    "    train[column] = le.transform(train_)\n",
    "    test[column] = le.transform(test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFifVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "with open(path_join(DATA_DIR, 'stopwords-ru.txt'), 'r') as file:\n",
    "    stop_words = file.read().split('\\n')\n",
    "    \n",
    "with open(path_join(DATA_DIR, 'stop-words-russian.txt'), 'r') as file:\n",
    "    stop_words += file.read().split('\\n')\n",
    "stop_words = set(stop_words)\n",
    "\n",
    "stop_words.discard('')\n",
    "stop_words.discard('\\ufeffа')\n",
    " \n",
    "stop_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=250000, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['отовсюду', 'всеми', 'важные', 'девять', 'свой', 'земля', 'двух', 'шесть', 'дать', 'вдруг', 'так', 'шестнадцать', 'недавно', 'что', 'тех', 'пор', 'других', 'наконец', 'ими', 'ночь', 'такая', 'пора', 'седьмой', 'место', 'этот', 'которого', 'двадцать', 'никакой', 'нее', 'советский', 'году'... 'старый', 'страна', 'свои', 'там', 'всё', 'его', 'ли', 'тобою', 'оказаться', 'каждый', 'миллионов'],\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = train['comment']\n",
    "test_text = test['comment']\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(max_features=250000,  \n",
    "                                  stop_words=stop_words,\n",
    "                                  analyzer='word',\n",
    "                                  ngram_range=(1,3),\n",
    "                                 )\n",
    "\n",
    "word_vectorizer.fit(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=2500, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['отовсюду', 'всеми', 'важные', 'девять', 'свой', 'земля', 'двух', 'шесть', 'дать', 'вдруг', 'так', 'шестнадцать', 'недавно', 'что', 'тех', 'пор', 'других', 'наконец', 'ими', 'ночь', 'такая', 'пора', 'седьмой', 'место', 'этот', 'которого', 'двадцать', 'никакой', 'нее', 'советский', 'году'... 'старый', 'страна', 'свои', 'там', 'всё', 'его', 'ли', 'тобою', 'оказаться', 'каждый', 'миллионов'],\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vectorizer =  TfidfVectorizer(max_features=2500,  \n",
    "                                  stop_words=stop_words,\n",
    "                                  analyzer='char',\n",
    "                                 )\n",
    "char_vectorizer.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_matrix(trainIndices, valIndices):\n",
    "    \"\"\" create sparse matr by wordVectorizer \"\"\"\n",
    "    train_word_features = word_vectorizer.transform(train.loc[trainIndices, 'comment'])\n",
    "    val_word_features = word_vectorizer.transform(train.loc[valIndices, 'comment'])\n",
    "        \n",
    "    return (train_word_features, val_word_features)\n",
    "\n",
    "def get_sparse_matrix_char(trainIndices, valIndices):\n",
    "    \"\"\" create sparse matrix by charVectorizer \"\"\"\n",
    "    train_word_features = char_vectorizer.transform(train.loc[trainIndices, 'comment'])\n",
    "    val_word_features = char_vectorizer.transform(train.loc[valIndices, 'comment'])\n",
    "        \n",
    "    return (train_word_features, val_word_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-01 00:22:55\n",
      "2017-10-31 23:55:45\n"
     ]
    }
   ],
   "source": [
    "print(train['date_created'].min())\n",
    "print(train['date_created'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation(df, start_date):\n",
    "    return (\n",
    "        df.loc[np.logical_and(df['date_created'] >= pd.to_datetime(start_date) - relativedelta(days=0),\n",
    "                              df['date_created'] <  pd.to_datetime(start_date) + relativedelta(months=6))].index,\n",
    "        df.loc[np.logical_and(df['date_created'] >= pd.to_datetime(start_date) + relativedelta(months=6),\n",
    "                              df['date_created'] <  pd.to_datetime(start_date) + relativedelta(months=10))].index\n",
    "           )\n",
    "\n",
    "train_dates = ['2016-01-01', '2016-06-01', '2017-01-01'] # split data to the 3 sub datasets\n",
    "\n",
    "myCViterator = []\n",
    "for i in train_dates:\n",
    "    trainIndices, valIndices = create_validation(train, i)\n",
    "\n",
    "    myCViterator.append([trainIndices, valIndices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 2016-01-01 - 2016-06-30, Y: 2016-07-01 - 2016-10-31\n",
      "X: 2016-06-01 - 2016-11-30, Y: 2016-12-01 - 2017-03-31\n",
      "X: 2017-01-01 - 2017-06-30, Y: 2017-07-01 - 2017-10-31\n"
     ]
    }
   ],
   "source": [
    "def get_date(df, index):\n",
    "    return df.loc[index, 'date_created'].date()\n",
    "\n",
    "for x, y in myCViterator:\n",
    "    print('X: {} - {}, Y: {} - {}'.format(get_date(train, min(x)), get_date(train, max(x)),\n",
    "                                          get_date(train, min(y)), get_date(train, max(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification based on message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c': 0.001, 'score': 0.6314092295368253}\n",
      "{'c': 0.01, 'score': 0.46912167358469564}\n",
      "{'c': 0.1, 'score': 0.2640957235860757}\n",
      "{'c': 1, 'score': 0.1520383191963193}\n",
      "{'c': 5, 'score': 0.12638545285138106}\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "params = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 5]\n",
    "}\n",
    "\n",
    "for c in params['C']:\n",
    "    \n",
    "    log_model = LogisticRegression(class_weight='balanced', random_state=42, C=c)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for trainIndex, valIndex in myCViterator:\n",
    "    \n",
    "        matrix = get_sparse_matrix(trainIndex, valIndex)\n",
    "        \n",
    "        log_model.fit(matrix[0], train.loc[trainIndex, 'is_fake'])\n",
    "        \n",
    "        prediction = log_model.predict_proba(matrix[1])\n",
    "        \n",
    "        scores.append(log_loss(train.loc[valIndex, 'is_fake'], prediction))\n",
    "        \n",
    "    models.append({'c': c, 'score': np.mean(scores)})\n",
    "\n",
    "for model in models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create new feature from the logReg predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>company_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_created</th>\n",
       "      <th>user_date_created</th>\n",
       "      <th>user_ip</th>\n",
       "      <th>user_agent</th>\n",
       "      <th>...</th>\n",
       "      <th>count_words_upper</th>\n",
       "      <th>count_words_title</th>\n",
       "      <th>word_unique_percent</th>\n",
       "      <th>weekend</th>\n",
       "      <th>month</th>\n",
       "      <th>ip_count_for_company</th>\n",
       "      <th>ip_count_for_product</th>\n",
       "      <th>id_count_for_company</th>\n",
       "      <th>mean_in_time</th>\n",
       "      <th>log_matrix_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1067612</td>\n",
       "      <td>1089378</td>\n",
       "      <td>4997365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Хороший магазин,быстрая доставка.</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-01-01 00:22:55</td>\n",
       "      <td>2016-01-01 00:22:55</td>\n",
       "      <td>91.192.132.251</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 4.4.2; LenovoA3300...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1067613</td>\n",
       "      <td>2364143</td>\n",
       "      <td>4997376</td>\n",
       "      <td>68308.0</td>\n",
       "      <td>Ребята молодцы!!отлично сработали, очень опера...</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-01-01 00:51:05</td>\n",
       "      <td>2016-01-01 00:51:05</td>\n",
       "      <td>79.140.1.116</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>375841</td>\n",
       "      <td>884214</td>\n",
       "      <td>1607869</td>\n",
       "      <td>318970.0</td>\n",
       "      <td>Кухня вся разломана, поцарапана и вообще не ра...</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 00:57:20</td>\n",
       "      <td>2013-10-13 13:06:58</td>\n",
       "      <td>194.54.160.10</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.3...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>95.454545</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1060723</td>\n",
       "      <td>2100809</td>\n",
       "      <td>4954901</td>\n",
       "      <td>902536.0</td>\n",
       "      <td>Швидко вийшли на зв'язок, вирішили усі організ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-01-01 01:15:38</td>\n",
       "      <td>2015-12-23 22:24:59</td>\n",
       "      <td>91.243.6.71</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 4.2.1; Lenovo P780...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1034089</td>\n",
       "      <td>2177682</td>\n",
       "      <td>4786680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Купил квадрокоптер. Прислали не в полной компл...</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-01 01:31:24</td>\n",
       "      <td>2015-11-28 23:58:27</td>\n",
       "      <td>188.163.84.118</td>\n",
       "      <td>Mozilla/5.0 (Android 4.2.2; Mobile; rv:41.0) G...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id  company_id  user_id  product_id  \\\n",
       "0     1067612     1089378  4997365         NaN   \n",
       "1     1067613     2364143  4997376     68308.0   \n",
       "2      375841      884214  1607869    318970.0   \n",
       "3     1060723     2100809  4954901    902536.0   \n",
       "4     1034089     2177682  4786680         NaN   \n",
       "\n",
       "                                             comment  rating  \\\n",
       "0                  Хороший магазин,быстрая доставка.       5   \n",
       "1  Ребята молодцы!!отлично сработали, очень опера...       5   \n",
       "2  Кухня вся разломана, поцарапана и вообще не ра...       1   \n",
       "3  Швидко вийшли на зв'язок, вирішили усі організ...       5   \n",
       "4  Купил квадрокоптер. Прислали не в полной компл...       2   \n",
       "\n",
       "         date_created   user_date_created         user_ip  \\\n",
       "0 2016-01-01 00:22:55 2016-01-01 00:22:55  91.192.132.251   \n",
       "1 2016-01-01 00:51:05 2016-01-01 00:51:05    79.140.1.116   \n",
       "2 2016-01-01 00:57:20 2013-10-13 13:06:58   194.54.160.10   \n",
       "3 2016-01-01 01:15:38 2015-12-23 22:24:59     91.243.6.71   \n",
       "4 2016-01-01 01:31:24 2015-11-28 23:58:27  188.163.84.118   \n",
       "\n",
       "                                          user_agent       ...         \\\n",
       "0  Mozilla/5.0 (Linux; Android 4.4.2; LenovoA3300...       ...          \n",
       "1  Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi...       ...          \n",
       "2  Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.3...       ...          \n",
       "3  Mozilla/5.0 (Linux; Android 4.2.1; Lenovo P780...       ...          \n",
       "4  Mozilla/5.0 (Android 4.2.2; Mobile; rv:41.0) G...       ...          \n",
       "\n",
       "   count_words_upper  count_words_title  word_unique_percent  weekend  month  \\\n",
       "0                  0                  1           100.000000        0      1   \n",
       "1                  0                  1           100.000000        0      1   \n",
       "2                  0                  3            95.454545        0      1   \n",
       "3                  1                  1           100.000000        0      1   \n",
       "4                  0                  4            95.000000        0      1   \n",
       "\n",
       "   ip_count_for_company  ip_count_for_product  id_count_for_company  \\\n",
       "0                   1.0                   1.0                     1   \n",
       "1                   1.0                   1.0                     1   \n",
       "2                   1.0                   1.0                     1   \n",
       "3                   1.0                   1.0                     1   \n",
       "4                   1.0                   1.0                     1   \n",
       "\n",
       "   mean_in_time  log_matrix_pred  \n",
       "0      0.010000              0.0  \n",
       "1      0.010000              0.0  \n",
       "2      0.010000              0.0  \n",
       "3      0.000297              0.0  \n",
       "4      0.000362              0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model = LogisticRegression(class_weight='balanced', random_state=42, C=1)\n",
    "predictions = []\n",
    "\n",
    "for trainIndex, valIndex in myCViterator:\n",
    "    matrix = get_sparse_matrix(trainIndex, valIndex)\n",
    "    \n",
    "    log_model.fit(matrix[0], train.loc[trainIndex, 'is_fake'])\n",
    "    \n",
    "    predictions.append([train.loc[valIndex, 'is_fake'], log_model.predict_proba(matrix[1])[:, 1]])\n",
    "    \n",
    "train[\"log_matrix_pred\"] = 0\n",
    "\n",
    "for i, fold in enumerate(myCViterator):\n",
    "    trainIndex, valIndex = fold[0], fold[1]\n",
    "    train.loc[valIndex, 'log_matrix_pred'] = predictions[i][1]\n",
    "    \n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>company_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_created</th>\n",
       "      <th>user_date_created</th>\n",
       "      <th>user_ip</th>\n",
       "      <th>user_agent</th>\n",
       "      <th>...</th>\n",
       "      <th>count_words_title</th>\n",
       "      <th>word_unique_percent</th>\n",
       "      <th>weekend</th>\n",
       "      <th>month</th>\n",
       "      <th>ip_count_for_company</th>\n",
       "      <th>ip_count_for_product</th>\n",
       "      <th>id_count_for_company</th>\n",
       "      <th>mean_in_time</th>\n",
       "      <th>log_matrix_pred</th>\n",
       "      <th>log_matrix_pred_char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1067612</td>\n",
       "      <td>1089378</td>\n",
       "      <td>4997365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Хороший магазин,быстрая доставка.</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-01-01 00:22:55</td>\n",
       "      <td>2016-01-01 00:22:55</td>\n",
       "      <td>91.192.132.251</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 4.4.2; LenovoA3300...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1067613</td>\n",
       "      <td>2364143</td>\n",
       "      <td>4997376</td>\n",
       "      <td>68308.0</td>\n",
       "      <td>Ребята молодцы!!отлично сработали, очень опера...</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-01-01 00:51:05</td>\n",
       "      <td>2016-01-01 00:51:05</td>\n",
       "      <td>79.140.1.116</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>375841</td>\n",
       "      <td>884214</td>\n",
       "      <td>1607869</td>\n",
       "      <td>318970.0</td>\n",
       "      <td>Кухня вся разломана, поцарапана и вообще не ра...</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 00:57:20</td>\n",
       "      <td>2013-10-13 13:06:58</td>\n",
       "      <td>194.54.160.10</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.3...</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>95.454545</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1060723</td>\n",
       "      <td>2100809</td>\n",
       "      <td>4954901</td>\n",
       "      <td>902536.0</td>\n",
       "      <td>Швидко вийшли на зв'язок, вирішили усі організ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-01-01 01:15:38</td>\n",
       "      <td>2015-12-23 22:24:59</td>\n",
       "      <td>91.243.6.71</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 4.2.1; Lenovo P780...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1034089</td>\n",
       "      <td>2177682</td>\n",
       "      <td>4786680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Купил квадрокоптер. Прислали не в полной компл...</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-01 01:31:24</td>\n",
       "      <td>2015-11-28 23:58:27</td>\n",
       "      <td>188.163.84.118</td>\n",
       "      <td>Mozilla/5.0 (Android 4.2.2; Mobile; rv:41.0) G...</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id  company_id  user_id  product_id  \\\n",
       "0     1067612     1089378  4997365         NaN   \n",
       "1     1067613     2364143  4997376     68308.0   \n",
       "2      375841      884214  1607869    318970.0   \n",
       "3     1060723     2100809  4954901    902536.0   \n",
       "4     1034089     2177682  4786680         NaN   \n",
       "\n",
       "                                             comment  rating  \\\n",
       "0                  Хороший магазин,быстрая доставка.       5   \n",
       "1  Ребята молодцы!!отлично сработали, очень опера...       5   \n",
       "2  Кухня вся разломана, поцарапана и вообще не ра...       1   \n",
       "3  Швидко вийшли на зв'язок, вирішили усі організ...       5   \n",
       "4  Купил квадрокоптер. Прислали не в полной компл...       2   \n",
       "\n",
       "         date_created   user_date_created         user_ip  \\\n",
       "0 2016-01-01 00:22:55 2016-01-01 00:22:55  91.192.132.251   \n",
       "1 2016-01-01 00:51:05 2016-01-01 00:51:05    79.140.1.116   \n",
       "2 2016-01-01 00:57:20 2013-10-13 13:06:58   194.54.160.10   \n",
       "3 2016-01-01 01:15:38 2015-12-23 22:24:59     91.243.6.71   \n",
       "4 2016-01-01 01:31:24 2015-11-28 23:58:27  188.163.84.118   \n",
       "\n",
       "                                          user_agent          ...           \\\n",
       "0  Mozilla/5.0 (Linux; Android 4.4.2; LenovoA3300...          ...            \n",
       "1  Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi...          ...            \n",
       "2  Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.3...          ...            \n",
       "3  Mozilla/5.0 (Linux; Android 4.2.1; Lenovo P780...          ...            \n",
       "4  Mozilla/5.0 (Android 4.2.2; Mobile; rv:41.0) G...          ...            \n",
       "\n",
       "   count_words_title  word_unique_percent  weekend  month  \\\n",
       "0                  1           100.000000        0      1   \n",
       "1                  1           100.000000        0      1   \n",
       "2                  3            95.454545        0      1   \n",
       "3                  1           100.000000        0      1   \n",
       "4                  4            95.000000        0      1   \n",
       "\n",
       "   ip_count_for_company  ip_count_for_product  id_count_for_company  \\\n",
       "0                   1.0                   1.0                     1   \n",
       "1                   1.0                   1.0                     1   \n",
       "2                   1.0                   1.0                     1   \n",
       "3                   1.0                   1.0                     1   \n",
       "4                   1.0                   1.0                     1   \n",
       "\n",
       "   mean_in_time  log_matrix_pred  log_matrix_pred_char  \n",
       "0      0.010000              0.0                   0.0  \n",
       "1      0.010000              0.0                   0.0  \n",
       "2      0.010000              0.0                   0.0  \n",
       "3      0.000297              0.0                   0.0  \n",
       "4      0.000362              0.0                   0.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model = LogisticRegression(class_weight='balanced', random_state=42, C=1)\n",
    "predictions = []\n",
    "\n",
    "for trainIndex, valIndex in myCViterator:\n",
    "    matrix = get_sparse_matrix_char(trainIndex, valIndex)\n",
    "    \n",
    "    log_model.fit(matrix[0], train.loc[trainIndex, 'is_fake'])\n",
    "    \n",
    "    predictions.append([train.loc[valIndex, 'is_fake'], log_model.predict_proba(matrix[1])[:, 1]])\n",
    "    \n",
    "train[\"log_matrix_pred_char\"] = 0\n",
    "\n",
    "for i, fold in enumerate(myCViterator):\n",
    "    trainIndex, valIndex = fold[0], fold[1]\n",
    "    train.loc[valIndex, 'log_matrix_pred_char'] = predictions[i][1]\n",
    "    \n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_id', 'company_id', 'user_id', 'product_id', 'comment',\n",
       "       'rating', 'date_created', 'user_date_created', 'user_ip', 'user_agent',\n",
       "       'is_fake', 'from_prev_comment', 'diff_time_comment_registration',\n",
       "       'counts', 'count_sent', 'count_words', 'count_unique_words',\n",
       "       'count_letters', 'count_punctuations', 'count_words_upper',\n",
       "       'count_words_title', 'word_unique_percent', 'weekend', 'month',\n",
       "       'ip_count_for_company', 'ip_count_for_product', 'id_count_for_company',\n",
       "       'mean_in_time', 'log_matrix_pred', 'log_matrix_pred_char'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostingClassifier validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for the GradientBoostingClassifier\n",
    "gb_features = ['rating', 'from_prev_comment', 'diff_time_comment_registration', 'counts', \n",
    "                'count_sent', 'count_words', 'count_unique_words', 'count_letters', \n",
    "                'count_punctuations', 'count_words_upper', 'count_words_title', 'weekend',\n",
    "                'ip_count_for_company', 'ip_count_for_product', 'id_count_for_company',\n",
    "                'month', \n",
    "                'log_matrix_pred',\n",
    "                'log_matrix_pred_char',\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.012201427499560017, 0.007834636632392172, 0.13641371371674274]\n",
      "{'c': 1, 'score': 0.052149925949564975}\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "params = {\n",
    "    'C': [1]\n",
    "}\n",
    "\n",
    "for c in params['C']:\n",
    "    \n",
    "    # log_model = LogisticRegression(class_weight='balanced', random_state=42, C=c)\n",
    "    gb = GradientBoostingClassifier(random_state=42)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for trainIndex, valIndex in myCViterator:\n",
    "    \n",
    "        x_train, x_val = train.loc[trainIndex, gb_features], train.loc[valIndex, gb_features]\n",
    "        y_train, y_val = train.loc[trainIndex, 'is_fake'], train.loc[valIndex, 'is_fake']\n",
    "        \n",
    "        gb.fit(x_train, y_train)\n",
    "        \n",
    "        prediction = gb.predict_proba(x_val)\n",
    "        \n",
    "        scores.append(log_loss(y_val, prediction))\n",
    "    \n",
    "    print(scores)\n",
    "    models.append({'c': c, 'score': np.mean(scores)})\n",
    "\n",
    "for model in models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // Validation is not valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train the models and give the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### choose the time range for the train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainIndex = train.loc[train['date_created'] >=  pd.to_datetime('2017-04-30')].index\n",
    "trainIndex = train.loc[train['date_created'] >=  pd.to_datetime('2016-01-01')].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create new feature from the logReg predictions in the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(class_weight='balanced', random_state=42, C=1)\n",
    "\n",
    "matrix = word_vectorizer.transform(train.loc[trainIndex, 'comment'])\n",
    "test_matrix = word_vectorizer.transform(test['comment'])\n",
    "\n",
    "log_model.fit(matrix, train.loc[trainIndex, 'is_fake'])\n",
    "\n",
    "log_predictions = log_model.predict_proba(test_matrix)[:, 1]\n",
    "\n",
    "test['log_matrix_pred'] = log_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(class_weight='balanced', random_state=42, C=1)\n",
    "\n",
    "matrix = char_vectorizer.transform(train.loc[trainIndex, 'comment'])\n",
    "test_matrix = char_vectorizer.transform(test['comment'])\n",
    "\n",
    "log_model.fit(matrix, train.loc[trainIndex, 'is_fake'])\n",
    "\n",
    "log_predictions = log_model.predict_proba(test_matrix)[:, 1]\n",
    "\n",
    "test['log_matrix_pred_char'] = log_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for the GradientBoostingClassifier\n",
    "gb_features = ['rating', 'from_prev_comment', 'diff_time_comment_registration', 'counts', \n",
    "                'count_sent', 'count_words', 'count_unique_words', 'count_letters', \n",
    "                'count_punctuations', 'count_words_upper', 'count_words_title', 'weekend',\n",
    "                'ip_count_for_company', 'ip_count_for_product', 'id_count_for_company',\n",
    "                'month', \n",
    "                'log_matrix_pred',\n",
    "                'log_matrix_pred_char',\n",
    "                ]\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "x_train, y_train = train.loc[trainIndex, gb_features], train.loc[trainIndex, 'is_fake']\n",
    "\n",
    "gb.fit(x_train, y_train)\n",
    "\n",
    "test['gb_pred'] = gb.predict_proba(test[gb_features])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for the training XGBClassifier\n",
    "\n",
    "needful_features = ['rating', 'from_prev_comment', 'diff_time_comment_registration', 'counts', \n",
    "                    'count_sent', 'count_words', 'count_unique_words', 'count_letters', 'count_punctuations',\n",
    "                    'count_words_upper', 'count_words_title', 'weekend', 'ip_count_for_company', \n",
    "                    'ip_count_for_product', 'id_count_for_company', \n",
    "                    'month', \n",
    "                    'log_matrix_pred', \n",
    "                    'log_matrix_pred_char',\n",
    "                    'gb_pred',\n",
    "#                   'os', 'device', 'browser', 'mean_in_time',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "x_train, y_train = train.loc[trainIndex, needful_features], train.loc[trainIndex, 'is_fake']\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "prediction = model.predict_proba(test[needful_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write the predictions into the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(test['comment_id'])\n",
    "result_df['is_fake'] = prediction[:, 1]\n",
    "\n",
    "result_df.to_csv(path_join(DATA_DIR, 'submission.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
