{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_profiling as pp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from os.path import join as path_join\n",
    "\n",
    "from user_agents import parse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/'\n",
    "\n",
    "train = pd.read_csv(path_join(DATA_DIR, 'train.csv'), parse_dates=['date_created', 'user_date_created'])\n",
    "test = pd.read_csv(path_join(DATA_DIR,'test.csv'), parse_dates=['date_created', 'user_date_created'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values(by='date_created', ascending=True)\n",
    "\n",
    "train = train.loc[train['date_created'] >= pd.to_datetime('2016-01-01')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating feature - difference between creating_comment_time and registration_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def time_difference_feature(df):\n",
    "    diff = pd.to_datetime(df['date_created']) - pd.to_datetime(df['user_date_created'])\n",
    "    result = diff.apply(lambda x: x.seconds // 60)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating feature - count of comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_comments(df):\n",
    "    '''merge return dataframe with train/test'''\n",
    "    result = pd.DataFrame(df.groupby('user_id').size().reset_index(name='counts'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dt_next = lambda ts: ts.diff().shift(1)\n",
    "train['from_prev_comment'] = train.groupby('user_id').date_created.apply(get_dt_next).dt.total_seconds() // 60\n",
    "\n",
    "test['from_prev_comment'] = test.groupby('user_id').date_created.apply(get_dt_next).dt.total_seconds() // 60\n",
    "\n",
    "train['from_prev_comment'].fillna(2500000, inplace=True)\n",
    "test['from_prev_comment'].fillna(2500000, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['diff_time_comment_registration'] = time_difference_feature(train)\n",
    "test['diff_time_comment_registration'] = time_difference_feature(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, count_comments(train), how='left', on='user_id')\n",
    "test = pd.merge(test, count_comments(test), how='left', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['count_sent'] = train[\"comment\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "train['count_words'] = train[\"comment\"].apply(lambda x: len(str(x).split()))\n",
    "train['count_unique_words'] = train[\"comment\"].apply(lambda x: len(set(str(x).split())))\n",
    "train['count_letters'] = train[\"comment\"].apply(lambda x: len(str(x)))\n",
    "train[\"count_punctuations\"] = train[\"comment\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "train[\"count_words_upper\"] = train[\"comment\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "train[\"count_words_title\"] = train[\"comment\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "train['word_unique_percent'] = train['count_unique_words'] * 100 / train['count_words']\n",
    "train[\"weekend\"] = ((train[\"date_created\"].dt.dayofweek) // 5 == 1).astype(int)\n",
    "train['month'] = train['date_created'].apply(lambda x: x.month)\n",
    "\n",
    "test['count_sent'] = test[\"comment\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "test['count_words'] = test[\"comment\"].apply(lambda x: len(str(x).split()))\n",
    "test['count_unique_words'] = test[\"comment\"].apply(lambda x: len(set(str(x).split())))\n",
    "test['count_letters'] = test[\"comment\"].apply(lambda x: len(str(x)))\n",
    "test[\"count_punctuations\"] = test[\"comment\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "test[\"count_words_upper\"] = test[\"comment\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test[\"count_words_title\"] = test[\"comment\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test['word_unique_percent'] = test['count_unique_words'] * 100 / test['count_words']\n",
    "test[\"weekend\"] = ((test[\"date_created\"].dt.dayofweek) // 5 == 1).astype(int)\n",
    "test['month'] = test['date_created'].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count amount of every ip for company\n",
    "ip_amount_for_company_train = train.groupby(['company_id', 'user_ip']).user_ip.count()\n",
    "ip_company_train = pd.DataFrame(ip_amount_for_company_train) \n",
    "ip_company_train.columns = ['ip_count_for_company']\n",
    "ip_company_train = ip_company_train.reset_index()\n",
    "train = pd.merge(train, ip_company_train, how='left', on=['company_id', 'user_ip'])\n",
    "train.ip_count_for_company.fillna(1, inplace = True)\n",
    "\n",
    "ip_amount_for_company_test = test.groupby(['company_id', 'user_ip']).user_ip.count()\n",
    "ip_company_test = pd.DataFrame(ip_amount_for_company_test) \n",
    "ip_company_test.columns = ['ip_count_for_company']\n",
    "ip_company_test = ip_company_test.reset_index()\n",
    "test = pd.merge(test, ip_company_test, how='left', on=['company_id', 'user_ip'])\n",
    "test.ip_count_for_company.fillna(1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count amount of every ip for product\n",
    "ip_amount_for_product_train = train.groupby(['product_id', 'user_ip']).user_ip.count()\n",
    "ip_product_train = pd.DataFrame(ip_amount_for_product_train) \n",
    "ip_product_train.columns = ['ip_count_for_product']\n",
    "ip_product_train = ip_product_train.reset_index()\n",
    "train = pd.merge(train, ip_product_train, how='left', on=['product_id', 'user_ip'])\n",
    "train.ip_count_for_product.fillna(1, inplace = True)\n",
    "\n",
    "ip_amount_for_product_test = test.groupby(['product_id', 'user_ip']).user_ip.count()\n",
    "ip_product_test = pd.DataFrame(ip_amount_for_product_test) \n",
    "ip_product_test.columns = ['ip_count_for_product']\n",
    "ip_product_test = ip_product_test.reset_index()\n",
    "test = pd.merge(test, ip_product_test, how='left', on=['product_id', 'user_ip'])\n",
    "test.ip_count_for_product.fillna(1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count amount of every id for company\n",
    "id_amount_for_company_train = train.groupby(['company_id', 'user_id']).user_id.count()\n",
    "id_company_train = pd.DataFrame(id_amount_for_company_train) \n",
    "id_company_train.columns = ['id_count_for_company']\n",
    "id_company_train = id_company_train.reset_index()\n",
    "train = pd.merge(train, id_company_train, how='left', on=['company_id', 'user_id'])\n",
    "train.id_count_for_company.fillna(1, inplace = True)\n",
    "\n",
    "id_amount_for_company_test = test.groupby(['company_id', 'user_id']).user_id.count()\n",
    "id_company_test = pd.DataFrame(id_amount_for_company_test) \n",
    "id_company_test.columns = ['id_count_for_company']\n",
    "id_company_test = id_company_test.reset_index()\n",
    "test = pd.merge(test, id_company_test, how='left', on=['company_id', 'user_id'])\n",
    "test.id_count_for_company.fillna(1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean comments ammount within min - max date_created\n",
    "\n",
    "train_user_min = pd.DataFrame(train.groupby(['user_id']).date_created.min())\n",
    "train_user_min.columns = ['min_date']\n",
    "train_user_min = train_user_min.reset_index()\n",
    "\n",
    "train_user_max = pd.DataFrame(train.groupby(['user_id']).date_created.max())\n",
    "train_user_max.columns = ['max_date']\n",
    "train_user_max = train_user_max.reset_index()\n",
    "\n",
    "train_user_diff_time = pd.merge(train_user_max, train_user_min, how='left', on='user_id')\n",
    "train_user_diff_time['diff_time'] = (train_user_diff_time.max_date - train_user_diff_time.min_date).dt.total_seconds()//(60*24)\n",
    "train_user_diff_time = pd.merge(train_user_diff_time, count_comments(train), how='left', on='user_id')\n",
    "train_user_diff_time['mean_in_time'] = train_user_diff_time.counts/(train_user_diff_time.diff_time+100)\n",
    "train_user_diff_time.drop(['max_date', 'min_date', 'diff_time', 'counts'], axis = 1, inplace = True)\n",
    "\n",
    "train = pd.merge(train, train_user_diff_time, how='left', on='user_id')\n",
    "\n",
    "\n",
    "test_user_min = pd.DataFrame(test.groupby(['user_id']).date_created.min())\n",
    "test_user_min.columns = ['min_date']\n",
    "test_user_min = test_user_min.reset_index()\n",
    "\n",
    "test_user_max = pd.DataFrame(test.groupby(['user_id']).date_created.max())\n",
    "test_user_max.columns = ['max_date']\n",
    "test_user_max = test_user_max.reset_index()\n",
    "\n",
    "test_user_diff_time = pd.merge(test_user_max, test_user_min, how='left', on='user_id')\n",
    "test_user_diff_time['diff_time'] = (test_user_diff_time.max_date - test_user_diff_time.min_date).dt.total_seconds()//(60*24)\n",
    "test_user_diff_time = pd.merge(test_user_diff_time, count_comments(test), how='left', on='user_id')\n",
    "test_user_diff_time['mean_in_time'] = test_user_diff_time.counts/(test_user_diff_time.diff_time+100)\n",
    "test_user_diff_time.drop(['max_date', 'min_date', 'diff_time', 'counts'], axis = 1, inplace = True)\n",
    "\n",
    "test = pd.merge(test, test_user_diff_time, how='left', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['user_agent'].fillna('None', inplace = True)\n",
    "test['user_agent'].fillna('None', inplace = True)\n",
    "\n",
    "ldevice = lambda x: x.device.brand\n",
    "los = lambda x: x.os.family\n",
    "lbrowser = lambda x: x.browser.family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ua = train['user_agent'].apply(parse)\n",
    "train['device'] = train_ua.apply(ldevice)\n",
    "train['os'] = train_ua.apply(los)\n",
    "train['browser'] = train_ua.apply(lbrowser)\n",
    "\n",
    "test_ua = train['user_agent'].apply(parse)\n",
    "test['device'] = test_ua.apply(ldevice)\n",
    "test['os'] = test_ua.apply(los)\n",
    "test['browser'] = test_ua.apply(lbrowser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "columns = ['os', 'device', 'browser']\n",
    "for column in columns:\n",
    "    train_ = train[column].astype(str).str.lower()\n",
    "    test_ = test[column].astype(str).str.lower()\n",
    "    train_test_union = set(train_).union(set(test_))\n",
    "    le.fit(list(train_test_union))\n",
    "    \n",
    "    train[column] = le.transform(train_)\n",
    "    test[column] = le.transform(test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFifVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "with open(path_join(DATA_DIR, 'stopwords-ru.txt'), 'r') as file:\n",
    "    stop_words = file.read().split('\\n')\n",
    "    \n",
    "with open(path_join(DATA_DIR, 'stop-words-russian.txt'), 'r') as file:\n",
    "    stop_words += file.read().split('\\n')\n",
    "stop_words = set(stop_words)\n",
    "\n",
    "stop_words.discard('')\n",
    "stop_words.discard('\\ufeffа')\n",
    " \n",
    "stop_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=250000, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['никто', 'этим', 'всюду', 'мор', 'вообще', 'нею', 'те', 'будто', 'русский', 'и', 'уже', 'рука', 'сама', 'слишком', 'наверху', 'бывь', 'этих', 'эта', 'вид', 'такое', 'тринадцатый', 'почти', 'ту', 'оно', 'мож', 'про', 'пятнадцатый', 'туда', 'назад', 'нельзя', 'сами', 'вдруг', 'голова', 'ше... 'мимо', 'хотя', 'им', 'дело', 'она', 'даром', 'низко', 'потом', 'утро', 'место', 'что', 'которого'],\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = train['comment']\n",
    "test_text = test['comment']\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(max_features=250000,  \n",
    "                                  stop_words=stop_words,\n",
    "                                  analyzer='word',\n",
    "                                  ngram_range=(1,3),\n",
    "                                 )\n",
    "\n",
    "word_vectorizer.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=2500, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['никто', 'этим', 'всюду', 'мор', 'вообще', 'нею', 'те', 'будто', 'русский', 'и', 'уже', 'рука', 'сама', 'слишком', 'наверху', 'бывь', 'этих', 'эта', 'вид', 'такое', 'тринадцатый', 'почти', 'ту', 'оно', 'мож', 'про', 'пятнадцатый', 'туда', 'назад', 'нельзя', 'сами', 'вдруг', 'голова', 'ше... 'мимо', 'хотя', 'им', 'дело', 'она', 'даром', 'низко', 'потом', 'утро', 'место', 'что', 'которого'],\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vectorizer =  TfidfVectorizer(max_features=2500,  \n",
    "                                  stop_words=stop_words,\n",
    "                                  analyzer='char',\n",
    "                                 )\n",
    "char_vectorizer.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_matrix(trainIndices, valIndices):\n",
    "    \"\"\" create sparse matr by wordVectorizer \"\"\"\n",
    "    train_word_features = word_vectorizer.transform(train.loc[trainIndices, 'comment'])\n",
    "    val_word_features = word_vectorizer.transform(train.loc[valIndices, 'comment'])\n",
    "        \n",
    "    return (train_word_features, val_word_features)\n",
    "\n",
    "def get_sparse_matrix_char(trainIndices, valIndices):\n",
    "    \"\"\" create sparse matrix by charVectorizer \"\"\"\n",
    "    train_word_features = char_vectorizer.transform(train.loc[trainIndices, 'comment'])\n",
    "    val_word_features = char_vectorizer.transform(train.loc[valIndices, 'comment'])\n",
    "        \n",
    "    return (train_word_features, val_word_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-01 00:22:55\n",
      "2017-10-31 23:55:45\n"
     ]
    }
   ],
   "source": [
    "print(train['date_created'].min())\n",
    "print(train['date_created'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation(df, start_date):\n",
    "    return (\n",
    "        df.loc[np.logical_and(df['date_created'] >= pd.to_datetime(start_date) - relativedelta(days=0),\n",
    "                              df['date_created'] <  pd.to_datetime(start_date) + relativedelta(months=6))].index,\n",
    "        df.loc[np.logical_and(df['date_created'] >= pd.to_datetime(start_date) + relativedelta(months=6),\n",
    "                              df['date_created'] <  pd.to_datetime(start_date) + relativedelta(months=10))].index\n",
    "           )\n",
    "\n",
    "train_dates = ['2016-01-01', '2016-06-01', '2017-01-01'] # split data to the 3 sub datasets\n",
    "\n",
    "myCViterator = []\n",
    "for i in train_dates:\n",
    "    trainIndices, valIndices = create_validation(train, i)\n",
    "\n",
    "    myCViterator.append([trainIndices, valIndices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 2016-01-01 - 2016-06-30, Y: 2016-07-01 - 2016-10-31\n",
      "X: 2016-06-01 - 2016-11-30, Y: 2016-12-01 - 2017-03-31\n",
      "X: 2017-01-01 - 2017-06-30, Y: 2017-07-01 - 2017-10-31\n"
     ]
    }
   ],
   "source": [
    "def get_date(df, index):\n",
    "    return df.loc[index, 'date_created'].date()\n",
    "\n",
    "for x, y in myCViterator:\n",
    "    print('X: {} - {}, Y: {} - {}'.format(get_date(train, min(x)), get_date(train, max(x)),\n",
    "                                          get_date(train, min(y)), get_date(train, max(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c': 0.001, 'score': 0.6314092295368253}\n",
      "{'c': 0.01, 'score': 0.46912167358469564}\n",
      "{'c': 0.1, 'score': 0.2640957235860757}\n",
      "{'c': 1, 'score': 0.1520383191963193}\n",
      "{'c': 5, 'score': 0.12638545285138106}\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "params = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 5]\n",
    "}\n",
    "\n",
    "for c in params['C']:\n",
    "    \n",
    "    log_model = LogisticRegression(class_weight='balanced', random_state=42, C=c)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for trainIndex, valIndex in myCViterator:\n",
    "    \n",
    "        matrix = get_sparse_matrix(trainIndex, valIndex)\n",
    "        \n",
    "        log_model.fit(matrix[0], train.loc[trainIndex, 'is_fake'])\n",
    "        \n",
    "        prediction = log_model.predict_proba(matrix[1])\n",
    "        \n",
    "        scores.append(log_loss(train.loc[valIndex, 'is_fake'], prediction))\n",
    "        \n",
    "    models.append({'c': c, 'score': np.mean(scores)})\n",
    "\n",
    "for model in models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create new feature from the logReg predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>company_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_created</th>\n",
       "      <th>user_date_created</th>\n",
       "      <th>user_ip</th>\n",
       "      <th>user_agent</th>\n",
       "      <th>...</th>\n",
       "      <th>count_words_title</th>\n",
       "      <th>word_unique_percent</th>\n",
       "      <th>weekend</th>\n",
       "      <th>month</th>\n",
       "      <th>ip_count_for_company</th>\n",
       "      <th>ip_count_for_product</th>\n",
       "      <th>id_count_for_company</th>\n",
       "      <th>mean_in_time</th>\n",
       "      <th>log_matrix_pred_char</th>\n",
       "      <th>log_matrix_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1067612</td>\n",
       "      <td>1089378</td>\n",
       "      <td>4997365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Хороший магазин,быстрая доставка.</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-01-01 00:22:55</td>\n",
       "      <td>2016-01-01 00:22:55</td>\n",
       "      <td>91.192.132.251</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 4.4.2; LenovoA3300...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1067613</td>\n",
       "      <td>2364143</td>\n",
       "      <td>4997376</td>\n",
       "      <td>68308.0</td>\n",
       "      <td>Ребята молодцы!!отлично сработали, очень опера...</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-01-01 00:51:05</td>\n",
       "      <td>2016-01-01 00:51:05</td>\n",
       "      <td>79.140.1.116</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>375841</td>\n",
       "      <td>884214</td>\n",
       "      <td>1607869</td>\n",
       "      <td>318970.0</td>\n",
       "      <td>Кухня вся разломана, поцарапана и вообще не ра...</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 00:57:20</td>\n",
       "      <td>2013-10-13 13:06:58</td>\n",
       "      <td>194.54.160.10</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.3...</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>95.454545</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1060723</td>\n",
       "      <td>2100809</td>\n",
       "      <td>4954901</td>\n",
       "      <td>902536.0</td>\n",
       "      <td>Швидко вийшли на зв'язок, вирішили усі організ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-01-01 01:15:38</td>\n",
       "      <td>2015-12-23 22:24:59</td>\n",
       "      <td>91.243.6.71</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 4.2.1; Lenovo P780...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1034089</td>\n",
       "      <td>2177682</td>\n",
       "      <td>4786680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Купил квадрокоптер. Прислали не в полной компл...</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-01 01:31:24</td>\n",
       "      <td>2015-11-28 23:58:27</td>\n",
       "      <td>188.163.84.118</td>\n",
       "      <td>Mozilla/5.0 (Android 4.2.2; Mobile; rv:41.0) G...</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id  company_id  user_id  product_id  \\\n",
       "0     1067612     1089378  4997365         NaN   \n",
       "1     1067613     2364143  4997376     68308.0   \n",
       "2      375841      884214  1607869    318970.0   \n",
       "3     1060723     2100809  4954901    902536.0   \n",
       "4     1034089     2177682  4786680         NaN   \n",
       "\n",
       "                                             comment  rating  \\\n",
       "0                  Хороший магазин,быстрая доставка.       5   \n",
       "1  Ребята молодцы!!отлично сработали, очень опера...       5   \n",
       "2  Кухня вся разломана, поцарапана и вообще не ра...       1   \n",
       "3  Швидко вийшли на зв'язок, вирішили усі організ...       5   \n",
       "4  Купил квадрокоптер. Прислали не в полной компл...       2   \n",
       "\n",
       "         date_created   user_date_created         user_ip  \\\n",
       "0 2016-01-01 00:22:55 2016-01-01 00:22:55  91.192.132.251   \n",
       "1 2016-01-01 00:51:05 2016-01-01 00:51:05    79.140.1.116   \n",
       "2 2016-01-01 00:57:20 2013-10-13 13:06:58   194.54.160.10   \n",
       "3 2016-01-01 01:15:38 2015-12-23 22:24:59     91.243.6.71   \n",
       "4 2016-01-01 01:31:24 2015-11-28 23:58:27  188.163.84.118   \n",
       "\n",
       "                                          user_agent       ...         \\\n",
       "0  Mozilla/5.0 (Linux; Android 4.4.2; LenovoA3300...       ...          \n",
       "1  Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi...       ...          \n",
       "2  Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.3...       ...          \n",
       "3  Mozilla/5.0 (Linux; Android 4.2.1; Lenovo P780...       ...          \n",
       "4  Mozilla/5.0 (Android 4.2.2; Mobile; rv:41.0) G...       ...          \n",
       "\n",
       "   count_words_title  word_unique_percent  weekend  month  \\\n",
       "0                  1           100.000000        0      1   \n",
       "1                  1           100.000000        0      1   \n",
       "2                  3            95.454545        0      1   \n",
       "3                  1           100.000000        0      1   \n",
       "4                  4            95.000000        0      1   \n",
       "\n",
       "   ip_count_for_company  ip_count_for_product  id_count_for_company  \\\n",
       "0                   1.0                   1.0                     1   \n",
       "1                   1.0                   1.0                     1   \n",
       "2                   1.0                   1.0                     1   \n",
       "3                   1.0                   1.0                     1   \n",
       "4                   1.0                   1.0                     1   \n",
       "\n",
       "   mean_in_time  log_matrix_pred_char  log_matrix_pred  \n",
       "0      0.010000                   0.0              0.0  \n",
       "1      0.010000                   0.0              0.0  \n",
       "2      0.010000                   0.0              0.0  \n",
       "3      0.000297                   0.0              0.0  \n",
       "4      0.000362                   0.0              0.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model = LogisticRegression(class_weight='balanced', random_state=42, C=1)\n",
    "predictions = []\n",
    "\n",
    "for trainIndex, valIndex in myCViterator:\n",
    "    matrix = get_sparse_matrix(trainIndex, valIndex)\n",
    "    \n",
    "    log_model.fit(matrix[0], train.loc[trainIndex, 'is_fake'])\n",
    "    \n",
    "    predictions.append([train.loc[valIndex, 'is_fake'], log_model.predict_proba(matrix[1])[:, 1]])\n",
    "    \n",
    "train[\"log_matrix_pred\"] = 0\n",
    "\n",
    "for i, fold in enumerate(myCViterator):\n",
    "    trainIndex, valIndex = fold[0], fold[1]\n",
    "    train.loc[valIndex, 'log_matrix_pred'] = predictions[i][1]\n",
    "    \n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>company_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>date_created</th>\n",
       "      <th>user_date_created</th>\n",
       "      <th>user_ip</th>\n",
       "      <th>user_agent</th>\n",
       "      <th>...</th>\n",
       "      <th>count_words_upper</th>\n",
       "      <th>count_words_title</th>\n",
       "      <th>word_unique_percent</th>\n",
       "      <th>weekend</th>\n",
       "      <th>month</th>\n",
       "      <th>ip_count_for_company</th>\n",
       "      <th>ip_count_for_product</th>\n",
       "      <th>id_count_for_company</th>\n",
       "      <th>mean_in_time</th>\n",
       "      <th>log_matrix_pred_char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1067612</td>\n",
       "      <td>1089378</td>\n",
       "      <td>4997365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Хороший магазин,быстрая доставка.</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-01-01 00:22:55</td>\n",
       "      <td>2016-01-01 00:22:55</td>\n",
       "      <td>91.192.132.251</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 4.4.2; LenovoA3300...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1067613</td>\n",
       "      <td>2364143</td>\n",
       "      <td>4997376</td>\n",
       "      <td>68308.0</td>\n",
       "      <td>Ребята молодцы!!отлично сработали, очень опера...</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-01-01 00:51:05</td>\n",
       "      <td>2016-01-01 00:51:05</td>\n",
       "      <td>79.140.1.116</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>375841</td>\n",
       "      <td>884214</td>\n",
       "      <td>1607869</td>\n",
       "      <td>318970.0</td>\n",
       "      <td>Кухня вся разломана, поцарапана и вообще не ра...</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-01 00:57:20</td>\n",
       "      <td>2013-10-13 13:06:58</td>\n",
       "      <td>194.54.160.10</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.3...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>95.454545</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1060723</td>\n",
       "      <td>2100809</td>\n",
       "      <td>4954901</td>\n",
       "      <td>902536.0</td>\n",
       "      <td>Швидко вийшли на зв'язок, вирішили усі організ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-01-01 01:15:38</td>\n",
       "      <td>2015-12-23 22:24:59</td>\n",
       "      <td>91.243.6.71</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 4.2.1; Lenovo P780...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1034089</td>\n",
       "      <td>2177682</td>\n",
       "      <td>4786680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Купил квадрокоптер. Прислали не в полной компл...</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-01 01:31:24</td>\n",
       "      <td>2015-11-28 23:58:27</td>\n",
       "      <td>188.163.84.118</td>\n",
       "      <td>Mozilla/5.0 (Android 4.2.2; Mobile; rv:41.0) G...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id  company_id  user_id  product_id  \\\n",
       "0     1067612     1089378  4997365         NaN   \n",
       "1     1067613     2364143  4997376     68308.0   \n",
       "2      375841      884214  1607869    318970.0   \n",
       "3     1060723     2100809  4954901    902536.0   \n",
       "4     1034089     2177682  4786680         NaN   \n",
       "\n",
       "                                             comment  rating  \\\n",
       "0                  Хороший магазин,быстрая доставка.       5   \n",
       "1  Ребята молодцы!!отлично сработали, очень опера...       5   \n",
       "2  Кухня вся разломана, поцарапана и вообще не ра...       1   \n",
       "3  Швидко вийшли на зв'язок, вирішили усі організ...       5   \n",
       "4  Купил квадрокоптер. Прислали не в полной компл...       2   \n",
       "\n",
       "         date_created   user_date_created         user_ip  \\\n",
       "0 2016-01-01 00:22:55 2016-01-01 00:22:55  91.192.132.251   \n",
       "1 2016-01-01 00:51:05 2016-01-01 00:51:05    79.140.1.116   \n",
       "2 2016-01-01 00:57:20 2013-10-13 13:06:58   194.54.160.10   \n",
       "3 2016-01-01 01:15:38 2015-12-23 22:24:59     91.243.6.71   \n",
       "4 2016-01-01 01:31:24 2015-11-28 23:58:27  188.163.84.118   \n",
       "\n",
       "                                          user_agent          ...           \\\n",
       "0  Mozilla/5.0 (Linux; Android 4.4.2; LenovoA3300...          ...            \n",
       "1  Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKi...          ...            \n",
       "2  Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.3...          ...            \n",
       "3  Mozilla/5.0 (Linux; Android 4.2.1; Lenovo P780...          ...            \n",
       "4  Mozilla/5.0 (Android 4.2.2; Mobile; rv:41.0) G...          ...            \n",
       "\n",
       "   count_words_upper  count_words_title  word_unique_percent  weekend  month  \\\n",
       "0                  0                  1           100.000000        0      1   \n",
       "1                  0                  1           100.000000        0      1   \n",
       "2                  0                  3            95.454545        0      1   \n",
       "3                  1                  1           100.000000        0      1   \n",
       "4                  0                  4            95.000000        0      1   \n",
       "\n",
       "   ip_count_for_company  ip_count_for_product  id_count_for_company  \\\n",
       "0                   1.0                   1.0                     1   \n",
       "1                   1.0                   1.0                     1   \n",
       "2                   1.0                   1.0                     1   \n",
       "3                   1.0                   1.0                     1   \n",
       "4                   1.0                   1.0                     1   \n",
       "\n",
       "   mean_in_time  log_matrix_pred_char  \n",
       "0      0.010000                   0.0  \n",
       "1      0.010000                   0.0  \n",
       "2      0.010000                   0.0  \n",
       "3      0.000297                   0.0  \n",
       "4      0.000362                   0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model = LogisticRegression(class_weight='balanced', random_state=42, C=1)\n",
    "predictions = []\n",
    "\n",
    "for trainIndex, valIndex in myCViterator:\n",
    "    matrix = get_sparse_matrix_char(trainIndex, valIndex)\n",
    "    \n",
    "    log_model.fit(matrix[0], train.loc[trainIndex, 'is_fake'])\n",
    "    \n",
    "    predictions.append([train.loc[valIndex, 'is_fake'], log_model.predict_proba(matrix[1])[:, 1]])\n",
    "    \n",
    "train[\"log_matrix_pred_char\"] = 0\n",
    "\n",
    "for i, fold in enumerate(myCViterator):\n",
    "    trainIndex, valIndex = fold[0], fold[1]\n",
    "    train.loc[valIndex, 'log_matrix_pred_char'] = predictions[i][1]\n",
    "    \n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_id', 'company_id', 'user_id', 'product_id', 'comment',\n",
       "       'rating', 'date_created', 'user_date_created', 'user_ip', 'user_agent',\n",
       "       'is_fake', 'from_prev_comment', 'diff_time_comment_registration',\n",
       "       'counts', 'count_sent', 'count_words', 'count_unique_words',\n",
       "       'count_letters', 'count_punctuations', 'count_words_upper',\n",
       "       'count_words_title', 'word_unique_percent', 'weekend', 'month',\n",
       "       'ip_count_for_company', 'ip_count_for_product', 'id_count_for_company',\n",
       "       'mean_in_time', 'log_matrix_pred_char'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for the training our models\n",
    "\n",
    "needful_features = ['rating', 'from_prev_comment', 'diff_time_comment_registration', 'counts', \n",
    "                    'count_sent', 'count_words', 'count_unique_words', 'count_letters', 'count_punctuations',\n",
    "                    'count_words_upper', 'count_words_title', 'weekend', 'ip_count_for_company', \n",
    "                    'ip_count_for_product', 'id_count_for_company', 'month', \n",
    "                    'log_matrix_pred',  'gb_pred',\n",
    "#                     'log_matrix_pred_char',\n",
    "#                   'os', 'device', 'browser', 'mean_in_time',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.012201427499560017, 0.007834636632392172, 0.13641371371674274]\n",
      "{'c': 1, 'score': 0.052149925949564975}\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "params = {\n",
    "    'C': [1]\n",
    "}\n",
    "\n",
    "for c in params['C']:\n",
    "    \n",
    "    # log_model = LogisticRegression(class_weight='balanced', random_state=42, C=c)\n",
    "    gb = GradientBoostingClassifier(random_state=42)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for trainIndex, valIndex in myCViterator:\n",
    "    \n",
    "        x_train, x_val = train.loc[trainIndex, needful_features], train.loc[valIndex, needful_features]\n",
    "        y_train, y_val = train.loc[trainIndex, 'is_fake'], train.loc[valIndex, 'is_fake']\n",
    "        \n",
    "        gb.fit(x_train, y_train)\n",
    "        \n",
    "        prediction = gb.predict_proba(x_val)\n",
    "        \n",
    "        scores.append(log_loss(y_val, prediction))\n",
    "    \n",
    "    print(scores)\n",
    "    models.append({'c': c, 'score': np.mean(scores)})\n",
    "\n",
    "for model in models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the time range for the train\n",
    "\n",
    "# trainIndex = train.loc[train['date_created'] >=  pd.to_datetime('2017-04-30')].index\n",
    "trainIndex = train.loc[train['date_created'] >=  pd.to_datetime('2016-01-01')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(class_weight='balanced', random_state=42, C=1)\n",
    "\n",
    "matrix = word_vectorizer.transform(train.loc[trainIndex, 'comment'])\n",
    "test_matrix = word_vectorizer.transform(test['comment'])\n",
    "\n",
    "log_model.fit(matrix, train.loc[trainIndex, 'is_fake'])\n",
    "\n",
    "log_predictions = log_model.predict_proba(test_matrix)[:, 1]\n",
    "\n",
    "test['log_matrix_pred'] = log_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(class_weight='balanced', random_state=42, C=1)\n",
    "\n",
    "matrix = char_vectorizer.transform(train.loc[trainIndex, 'comment'])\n",
    "test_matrix = char_vectorizer.transform(test['comment'])\n",
    "\n",
    "log_model.fit(matrix, train.loc[trainIndex, 'is_fake'])\n",
    "\n",
    "log_predictions = log_model.predict_proba(test_matrix)[:, 1]\n",
    "\n",
    "test['log_matrix_pred_char'] = log_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for the GradientBoostingClassifier\n",
    "gb_features = ['rating', 'from_prev_comment', 'diff_time_comment_registration', 'counts', \n",
    "                'count_sent', 'count_words', 'count_unique_words', 'count_letters', \n",
    "                'count_punctuations', 'count_words_upper', 'count_words_title', 'weekend',\n",
    "                'ip_count_for_company', 'ip_count_for_product', 'id_count_for_company',\n",
    "                'month', 'log_matrix_pred',\n",
    "#                 'log_matrix_pred_char',\n",
    "                ]\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "x_train, y_train = train.loc[trainIndex, gb_features], train.loc[trainIndex, 'is_fake']\n",
    "\n",
    "gb.fit(x_train, y_train)\n",
    "\n",
    "test['gb_pred'] = gb.predict_proba(test[gb_features])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "x_train, y_train = train.loc[trainIndex, needful_features], train.loc[trainIndex, 'is_fake']\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "prediction = model.predict_proba(test[needful_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(test['comment_id'])\n",
    "result_df['is_fake'] = prediction[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkDF = pd.read_csv(path_join(DATA_DIR, 'best_submition/submission_RIt.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.444795</td>\n",
       "      <td>0.008649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.032736</td>\n",
       "      <td>0.009089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017183</td>\n",
       "      <td>0.011847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.013221</td>\n",
       "      <td>0.006267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.013221</td>\n",
       "      <td>0.006880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.013221</td>\n",
       "      <td>0.006375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.465849</td>\n",
       "      <td>0.008751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.013221</td>\n",
       "      <td>0.005825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.020412</td>\n",
       "      <td>0.010306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.013349</td>\n",
       "      <td>0.008018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.000802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.000745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.002418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.005919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.007352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.002007</td>\n",
       "      <td>0.000665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.000921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.001768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.003414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252069</th>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252070</th>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.000828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252071</th>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.003457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252072</th>\n",
       "      <td>0.003651</td>\n",
       "      <td>0.003977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252073</th>\n",
       "      <td>0.003038</td>\n",
       "      <td>0.004312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252074</th>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252075</th>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.001112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252076</th>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.001044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252077</th>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.000828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252078</th>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252079</th>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.002907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252080</th>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.002838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252081</th>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.004854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252082</th>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.004222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252083</th>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.005164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252084</th>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252085</th>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.000541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252086</th>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.006225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252087</th>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.008344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252088</th>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252089</th>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252090</th>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.000589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252091</th>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252092</th>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252093</th>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.001044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252094</th>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.000398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252095</th>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252096</th>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.005284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252097</th>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.007694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252098</th>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.001044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252099 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            from        to\n",
       "0       0.000241  0.000828\n",
       "1       0.000338  0.000575\n",
       "2       0.444795  0.008649\n",
       "3       0.032736  0.009089\n",
       "4       0.017183  0.011847\n",
       "5       0.013221  0.006267\n",
       "6       0.013221  0.006880\n",
       "7       0.013221  0.006375\n",
       "8       0.465849  0.008751\n",
       "9       0.013221  0.005825\n",
       "10      0.020412  0.010306\n",
       "11      0.013349  0.008018\n",
       "12      0.000100  0.000250\n",
       "13      0.000634  0.000802\n",
       "14      0.000634  0.000745\n",
       "15      0.000399  0.002418\n",
       "16      0.000350  0.000281\n",
       "17      0.000211  0.000789\n",
       "18      0.000833  0.005919\n",
       "19      0.000724  0.007352\n",
       "20      0.000148  0.000168\n",
       "21      0.000064  0.000230\n",
       "22      0.000593  0.000734\n",
       "23      0.000338  0.000741\n",
       "24      0.000593  0.000828\n",
       "25      0.002007  0.000665\n",
       "26      0.001547  0.000921\n",
       "27      0.000363  0.001768\n",
       "28      0.000599  0.003414\n",
       "29      0.000329  0.000235\n",
       "...          ...       ...\n",
       "252069  0.000399  0.000877\n",
       "252070  0.000713  0.000828\n",
       "252071  0.005033  0.003457\n",
       "252072  0.003651  0.003977\n",
       "252073  0.003038  0.004312\n",
       "252074  0.000593  0.000828\n",
       "252075  0.000502  0.001112\n",
       "252076  0.000211  0.001044\n",
       "252077  0.000634  0.000828\n",
       "252078  0.000502  0.000789\n",
       "252079  0.000819  0.002907\n",
       "252080  0.000514  0.002838\n",
       "252081  0.000555  0.004854\n",
       "252082  0.000616  0.004222\n",
       "252083  0.000721  0.005164\n",
       "252084  0.000782  0.000563\n",
       "252085  0.000461  0.000541\n",
       "252086  0.000311  0.006225\n",
       "252087  0.000311  0.008344\n",
       "252088  0.000394  0.000261\n",
       "252089  0.000310  0.000797\n",
       "252090  0.001029  0.000589\n",
       "252091  0.000593  0.000926\n",
       "252092  0.000338  0.000914\n",
       "252093  0.000593  0.001044\n",
       "252094  0.000768  0.000398\n",
       "252095  0.000311  0.000601\n",
       "252096  0.000768  0.005284\n",
       "252097  0.000960  0.007694\n",
       "252098  0.000593  0.001044\n",
       "\n",
       "[252099 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference_locations = np.where(resuld_df != checkDF)\n",
    "changed_from = result_df.values[difference_locations] \n",
    "changed_to = checkDF.values[difference_locations]\n",
    "pd.DataFrame({'from': changed_from, 'to': changed_to})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(test['comment_id'])\n",
    "result_df['is_fake'] = prediction[:, 1]\n",
    "\n",
    "result_df.to_csv(path_join(DATA_DIR, 'submission.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
